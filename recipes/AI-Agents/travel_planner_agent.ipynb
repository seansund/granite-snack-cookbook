{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Travel planner agentic system using the Granite-3-8B-Instruct model in watsonx.ai\n",
    "\n",
    "\n",
    "**Author**: Manoj Jahgirdar\n",
    "\n",
    "In this tutorial, you will create a Travel planner agent with LangChain agents using the IBM [Granite-3-8B-Instruct model](https://www.ibm.com/granite) now available on [watsonx.ai](https://www.ibm.com/products/watsonx-ai) that can answer complex queries about planning a trip.\n",
    "\n",
    "\n",
    "# Overview of the use case\n",
    "\n",
    "## What is the problem?\n",
    "Planning a trip can be a daunting task, especially when you have to consider various factors such as weather, attractions, and transportation. A travel planner agent can help you plan your trip by providing personalized recommendations based on your preferences and constraints. A normal llm cannot get current events and weather information. We'll see one such example where the llm fails suggests places that are closed recently and is completely clueless about the weather.\n",
    "\n",
    "\n",
    "## What are AI agents?\n",
    "\n",
    "At the core of this use case is [artificial intelligence (AI)](https://www.ibm.com/topics/artificial-intelligence) agents. An [AI agent](https://www.ibm.com/think/topics/ai-agents) is a system that uses LLM to understand “the path to follow” in order to answer a user’s query accurately. Since LLMs are becoming good at reasoning, they can be leveraged to \"create a plan of action\" and execute the plan of action by allowing the agent to  use a set of tools. In this use case, the agent uses Wikipedia and OpenMeteo tools to get the current events and weather information to provide a more personalized answer.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "You need an [IBM Cloud® account](https://cloud.ibm.com/registration) to create a [watsonx.ai™](https://www.ibm.com/products/watsonx-ai) project.\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. \n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "\tYou can get your project ID from within your project. Click the **Manage** tab. Then, copy the project ID from the **Details** section of the **General** page. You need this ID for this tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
    "\n",
    "This step will open a Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset.\n",
    "\n",
    "## Step 2. Set up a Watson Machine Learning (WML) service instance and API key.\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning) service instance (select your appropriate region and choose the Lite plan, which is a free instance).\n",
    "\n",
    "\n",
    "2. Generate an [API Key in WML](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html). \n",
    "\n",
    "\n",
    "3. Associate the WML service to the project that you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html). \n",
    "\n",
    "\n",
    "## Step 3. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip installation. \n",
    "\n",
    "Common Python frameworks for building agentic RAG systems include LangChain and LlamaIndex. In this tutorial, we will be using LangChain.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da058c8f-6137-4eb1-bbfb-7e6025e0585e"
   },
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install -q git+https://github.com/ibm-granite-community/utils \\\n",
    "    langchain \\\n",
    "    langchain-ibm \\\n",
    "    langchain_community \\\n",
    "    ibm-watsonx-ai \\\n",
    "    ibm_watson_machine_learning \\\n",
    "    wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d03aece6-a375-44a9-863c-ed44ef48f97a"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f930147-a42f-4951-9959-34b1379e4494"
   },
   "source": [
    "Set up your credentials. Please store your `PROJECT_ID` and `APIKEY` in a separate `.env` file in the same level of your directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d918935c-2176-48bb-be63-b5f9d00ad9b7"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "credentials = {\n",
    "    \"url\": get_env_var(\"WATSONX_URL\"),\n",
    "    \"apikey\": get_env_var(\"WATSONX_APIKEY\")\n",
    "}\n",
    "project_id = get_env_var(\"WATSONX_PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Initialization a basic agent with no tools\n",
    "\n",
    "This step is important as it will produce a clear example of an agent's behavior with and without external data sources. Let's start by setting our parameters.\n",
    "\n",
    "The model parameters available can be found [here](https://ibm.github.io/watson-machine-learning-sdk/model.html). We experimented with various model parameters, including temperature, minimum and maximum new tokens and stop sequences. Learn more about model parameters and what they mean in the [watsonx docs](https://www.ibm.com/docs/en/watsonx/saas). It is important to set our `stop_sequences` here in order to limit agent hallucinations. This tells the agent to stop producing further output upon encountering particular substrings. In our case, we want the agent to end its response upon reaching an observation and to not hallucinate a human response. Hence, one of our stop_sequences is `'Human:'` and another is `Observation` to halt once a final response is produced.\n",
    "\n",
    "For this tutorial, we suggest using IBM's Granite 3 8B Instruct model as the LLM to achieve similar results. You are free to use any AI model of your choice. The foundation models available through watsonx can be found [here](https://www.ibm.com/products/watsonx-ai/foundation-models). The purpose of these models in LLM applications is to serve as the reasoning engine that decides which actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f03cba7-d38f-41c1-89de-f2fdcc84e129"
   },
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params={\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.MIN_NEW_TOKENS: 5,\n",
    "        GenParams.MAX_NEW_TOKENS: 800,\n",
    "        GenParams.STOP_SEQUENCES: [\"Human:\", \"Observation\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a prompt template in case you want to ask multiple questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8ea4a30-b26b-436b-881c-9d5bd0e11e01"
   },
   "outputs": [],
   "source": [
    "template = \"Answer the {input} accurately. If you do not know the answer, simply say you do not know.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our prompt and our LLM. This allows the generative model to produce a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14d6be56-3528-4047-b294-e8679f85adef"
   },
   "outputs": [],
   "source": [
    "agent = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to see how our agent responds to a basic query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "396c7258-8da2-4db5-b9a7-cd9320c3fb4b"
   },
   "outputs": [],
   "source": [
    "print(agent.invoke({\"input\": \"How is the weather in New York?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent failed to answer the above query as it cannot get the current events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79b183e5-1f97-4969-9c60-be96e7553dd0"
   },
   "outputs": [],
   "source": [
    "print(agent.invoke({\"input\": \"I am planning a trip to New York City next week. Can you gather information about the best places to visit, provide a weather forecast, and recommend activities based on current events and conditions?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, The LLM gave a generic response and to its knowledge it was last updated upon and also hallucinated a response. In the next session lets solve the knowledge cut off problem and build a system around the LLM so that it gives us more personalized answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Setup Tools\n",
    "\n",
    "The main tools required for the agentic system to get the current events. The tools are:\n",
    "- **Wikipedia Tool:** To get the attractions\n",
    "- **OpenMeteo Tool:** To get the weather details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61641fbf-e4e9-4063-8083-dc587f1e9a5b"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "\n",
    "class WikipediaTool:\n",
    "    def get_information(self, query):\n",
    "        wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "        response = wikipedia.run(query)\n",
    "        return response\n",
    "\n",
    "    def get_tool(self):\n",
    "        return Tool.from_function(\n",
    "            func=self.get_information,\n",
    "            name=\"WikipediaTool\",\n",
    "            description=\"Use this tool to get attractions of a city.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the documents using LangChain `WebBaseLoader` for the URLs we listed. We'll also print a sample document to see how it loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dae54a55-838b-4e50-9d69-805f50764793"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote  # For URL encoding city names\n",
    "from langchain.tools import Tool\n",
    "\n",
    "class OpenMeteoTool:\n",
    "    def get_coordinates(self, city_name):\n",
    "        # URL encode the city name to handle spaces and special characters\n",
    "        encoded_city_name = quote(city_name)\n",
    "\n",
    "        geocode_url = f\"https://nominatim.openstreetmap.org/search?q={encoded_city_name}&format=json\"\n",
    "\n",
    "        headers = {\n",
    "            'User-Agent': 'MyWeatherApp/1.0 (Geocoding and Weather Service)'  # Generic app description\n",
    "        }\n",
    "\n",
    "        response = requests.get(geocode_url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            if data:\n",
    "                latitude = data[0].get('lat')\n",
    "                longitude = data[0].get('lon')\n",
    "\n",
    "                # Ensure latitude and longitude were found\n",
    "                if latitude and longitude:\n",
    "                    return latitude, longitude\n",
    "                else:\n",
    "                    raise ValueError(f\"Coordinates not found for '{city_name}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"No data returned for city '{city_name}'.\")\n",
    "        else:\n",
    "            raise Exception(f\"Nominatim API returned an error: {response.status_code}\")\n",
    "\n",
    "    def get_weather(self, city_name):\n",
    "        try:\n",
    "            # Fetch coordinates using the get_coordinates method\n",
    "            lat, lon = self.get_coordinates(city_name)\n",
    "\n",
    "            # Fetch weather information from Open-Meteo API using the coordinates\n",
    "            weather_url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "            weather_response = requests.get(weather_url)\n",
    "\n",
    "            if weather_response.status_code == 200:\n",
    "                weather_data = weather_response.json()\n",
    "                current_weather = weather_data.get('current_weather')\n",
    "\n",
    "                if current_weather:\n",
    "                    response = f\"Current temperature in {city_name} is {current_weather['temperature']}°C, with wind speed of {current_weather['windspeed']} m/s and it is { 'day' if current_weather['is_day'] == 1 else 'night'} time.\"\n",
    "                    return response\n",
    "                else:\n",
    "                    raise Exception(\"Weather data not available.\")\n",
    "            else:\n",
    "                raise Exception(f\"Open-Meteo API returned an error: {weather_response.status_code}\")\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "\n",
    "    def get_tool(self):\n",
    "        return Tool.from_function(\n",
    "            func=self.get_weather,\n",
    "            name=\"OpenMeteoTool\",\n",
    "            description=\"Use this tool to get weather information of a city.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e46ad94-cc46-4c53-b25c-2aa0d86b908f"
   },
   "outputs": [],
   "source": [
    "tools = [WikipediaTool().get_tool(), OpenMeteoTool().get_tool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Establish the prompt template\n",
    "\n",
    "Next, we will set up a new prompt template to ask multiple questions. This template is more complex. It is referred to as a [structured chat prompt](https://api.python.langchain.com/en/latest/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html#langchain-agents-structured-chat-base-create-structured-chat-agent) and can be used for creating agents that have multiple tools available. In our case, the tool we are using was defined in Step 5. The structured chat prompt will be made up of a `system_prompt`, a `human_prompt` and our tools. \n",
    "\n",
    "First, we will set up the `system_prompt`. This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "231e3aee-6348-496d-b00e-bf36209f34c2"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<|start_of_role|>system<|end_of_role|>Respond to the human as helpfully and accurately as possible.\n",
    "You have access to the following tools:<|end_of_text|>\n",
    "<|start_of_role|>tools<|end_of_role|>\n",
    "{tools}\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>system<|end_of_role|>\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\"\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "ALways Remember to respond in a structured manner with proper bullet points and lists.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n",
    "<|end_of_text|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we are establishing the `human_prompt`. This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2043dff2-df2a-4e53-b474-fd3e59177458"
   },
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"<|start_of_role|>user<|end_of_role|>{input}<|end_of_text|>\n",
    "{agent_scratchpad}\n",
    "(reminder to always respond in a JSON blob)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_prompt = \"\"\"<|start_of_role|>assistant<|end_of_role|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish the order of our newly defined prompts in the prompt template. We create this new template to feature the `system_prompt` followed by an optional list of messages collected in the agent's memory, if any, and finally, the `human_prompt` which includes both the human input and `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1941e00f-ff7e-46ae-b132-10bbf03dc2ad"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "        (\"assistant\", assistant_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's finalize our prompt template by adding the tool names, descriptions and arguments using a [partial prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/). This allows the agent to access the information pertaining to each tool including the intended use cases and also means we can add and remove tools without altering our entire prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f30dc8f-8754-4e8f-a88b-a4ce435c790e"
   },
   "outputs": [],
   "source": [
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Set up the agent's memory and chain\n",
    "\n",
    "An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's `ChatMessageHistory` that simply store chat history in the prompt and send it with every user question to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80ece439-9f5f-4430-8ab7-ee0c7cd1beb4"
   },
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The AgentExecutor class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85b8a2a6-6711-47db-a778-f975dea90468"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor_chat = AgentExecutor(\n",
    "    agent=chain, tools=tools, handle_parsing_errors=True, verbose=True\n",
    ")\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor_chat,\n",
    "    get_session_history=lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Generate responses with the travel planner agent\n",
    "\n",
    "We are now able to ask the agent questions. Recall the agent's previous inability to provide us with information pertaining to the weather and attractions. Now that the agent has its tools available to use, let's try asking the same questions again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae01f307-246e-4b5f-a3af-ccb4171ee39c"
   },
   "outputs": [],
   "source": [
    "answer1 = agent_with_chat_history.invoke(\n",
    "        {\"input\": \"How is the weather in New York?\"},\n",
    "        config={\"configurable\": {\"session_id\": \"watsonx\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb1477a1-ebeb-4e36-9520-465384bd9aea"
   },
   "outputs": [],
   "source": [
    "print(answer1['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The agent used its available OpenMeteo tool to return the weather conditions of New York!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9041310a-cdca-4793-869c-b905388cb235"
   },
   "outputs": [],
   "source": [
    "answer2 = agent_with_chat_history.invoke(\n",
    "        {\"input\": \"I am planning a trip to New York next week. Can you gather information about the best places to visit, provide a weather forecast, and recommend activities based on current events and conditions?\"},\n",
    "        config={\"configurable\": {\"session_id\": \"watsonx\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7259ea0b-b866-4523-9e35-15fae1bce2e7"
   },
   "outputs": [],
   "source": [
    "print(answer2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the agent used its Wikipedia tool to get the top 10 attractions and OpenMeteo tool to get the weather information. As a result hallucination is reduced and the agent is able to provide a more personalized answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you created a travel planner agent using LangChain in python with watsonx. The LLM you worked with was the IBM Granite-3-8B-Instruct model. The sample output is important as it shows the significance of this [generative AI](https://www.ibm.com/topics/generative-ai) advancement. The AI agent was successfully able to retrieve relevant information via the `WikipediaTool` and `OpenMeteoTool` tools, update its memory with each interaction and output appropriate responses. It is also important to note the agent's ability to determine whether tool calling is appropriate for each specific task. When the agent had the information necessary to answer the input query, it did not use any tools for question answering. \n",
    "\n",
    "For more AI agent content, we encourage you to check out our [Use watsonx.ai and LangChain Agents to perform E-commerce Analytics](https://github.com/manojjahgirdar/ibm-granite-recipes/blob/main/LLM_Agent_for_E-commerce_Analytics.ipynb) This Agentic solution for e-commerce analytics provides businesses with actionable insights into customer behavior and product interactions. The agent enables a deeper understanding of purchasing patterns and customer sentiment. It empowers businesses to enhance personalization, optimize product offerings, and improve customer satisfaction by delivering tailored insights that drive informed decision-making. This solution transforms raw data into valuable business intelligence, helping companies stay competitive and responsive to customer needs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
