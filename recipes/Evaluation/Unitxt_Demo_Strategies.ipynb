{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Demo Selection Strategies\n",
    "\n",
    "_Note_: for an introduction to model evaluation, see the [Quick Start](Unitxt_Quick_Start.ipynb) Cookbook.\n",
    "\n",
    "In this example, we experiment with different demo selection strategies in a classification task to determine which is best for our needs. We can easily change the number of shots provided with each test prompt, and the strategy for selecting the shots provided.\n",
    "\n",
    "We use the [Ledgar](https://www.unitxt.ai/en/latest/catalog/catalog.cards.ledgar.html) dataset from the Unitxt catalog as basis for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install replicate\n",
    "%pip install unitxt\n",
    "%pip install openai\n",
    "%pip install litellm\n",
    "%pip install diskcache\n",
    "%pip install tenacity\n",
    "%pip install tabulate\n",
    "%pip install scikit-learn\n",
    "%pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitxt.api import evaluate, load_dataset\n",
    "from unitxt.inference import CrossProviderInferenceEngine\n",
    "from unitxt.splitters import CloseTextSampler, FixedIndicesSampler, RandomSampler\n",
    "\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure demo selection strategies to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSampler - randomly sample a different set of examples for each test instance\n",
    "# CloseTextSampler - select the lexically closest samples from the demo pool for each test instance\n",
    "# FixedIndicesSampler - select the same fixed set of demo examples for all instances\n",
    "samplers = [\n",
    "    RandomSampler(),\n",
    "    CloseTextSampler(field=\"text\"),\n",
    "    FixedIndicesSampler(indices=[0,1])\n",
    "    ]\n",
    "\n",
    "# Different number of shots to evaluate\n",
    "number_of_demos = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the evaluation client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossProviderInferenceEngine(model=\"granite-3-8b-instruct\", provider=\"replicate\",credentials={'api_token': get_env_var('REPLICATE_API_TOKEN')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through the different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"num_demos\", \"sampler\", \"f1_micro\", \"ci_low\", \"ci_high\"])\n",
    "\n",
    "for num_demos in number_of_demos:\n",
    "    for demo_sampler in samplers:\n",
    "        dataset = load_dataset(\n",
    "            card=\"cards.ledgar\",\n",
    "            template=\"templates.classification.multi_class.title\",\n",
    "            format=\"formats.chat_api\",\n",
    "            num_demos=num_demos,\n",
    "            demos_pool_size=50,\n",
    "            loader_limit=200,\n",
    "            max_test_instances=10,\n",
    "            sampler=demo_sampler,\n",
    "            split=\"test\",\n",
    "        )\n",
    "\n",
    "        predictions = model(dataset)\n",
    "        results = evaluate(predictions=predictions, data=dataset)\n",
    "\n",
    "        global_scores = results.global_scores\n",
    "\n",
    "        df.loc[len(df)] = [\n",
    "            num_demos,\n",
    "            demo_sampler.__type__,\n",
    "            global_scores[\"score\"],\n",
    "            global_scores[\"score_ci_low\"],\n",
    "            global_scores[\"score_ci_high\"],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.round(decimals=2)\n",
    "print(df.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
