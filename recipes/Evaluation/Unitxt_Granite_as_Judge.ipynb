{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Granite as Judge\n",
    "\n",
    "_Note_: for an introduction to model evaluation, see the [Quick Start](Unitxt_Quick_Start.ipynb) Cookbook.\n",
    "\n",
    "In this example, we use Granite as an evaluator of predictions created by another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install replicate\n",
    "%pip install unitxt\n",
    "%pip install openai\n",
    "%pip install litellm\n",
    "%pip install diskcache\n",
    "%pip install tenacity\n",
    "%pip install tabulate\n",
    "%pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitxt.api import evaluate, create_dataset\n",
    "from unitxt.inference import CrossProviderInferenceEngine\n",
    "from unitxt.llm_as_judge import LLMJudgeDirect, DirectCriteriaCatalogEnum\n",
    "\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the sample data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"question\": \"Who is Harry Potter?\"},\n",
    "    {\"question\": \"How can I protect myself from the wind while walking outside?\"},\n",
    "    {\"question\": \"What is a good low cost of living city in the US?\"},\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    \"\"\"Harry Potter is a young wizard who becomes famous for surviving an attack by the dark wizard Voldemort, and later embarks on a journey to defeat him and uncover the truth about his past.\"\"\",\n",
    "    \"\"\"You can protect yourself from the wind by wearing windproof clothing, layering up, and using accessories like hats, scarves, and gloves to cover exposed skin.\"\"\",\n",
    "    \"\"\"A good low-cost-of-living city in the U.S. is San Francisco, California, known for its affordable housing and budget-friendly lifestyle.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the judge metric\n",
    "\n",
    "We would like to evaluate how relevant the answer is to the question asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = LLMJudgeDirect(\n",
    "    evaluator_name=\"GRANITE3_3_8B\",\n",
    "    inference_engine=CrossProviderInferenceEngine(\n",
    "        model=\"replicate/ibm-granite/granite-3.3-8b-instruct\",\n",
    "        provider=\"replicate\",\n",
    "        credentials={'api_token': get_env_var('REPLICATE_API_TOKEN')},\n",
    "        provider_specific_args={'max_requests_per_second': 1}\n",
    "    ),\n",
    "    criteria=DirectCriteriaCatalogEnum.ANSWER_RELEVANCE.value,\n",
    "    context_fields=[\"question\"],\n",
    "    criteria_field=\"criteria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(\n",
    "    task=\"tasks.qa.open\",\n",
    "    test_set=data,\n",
    "    metrics=[metric],\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(predictions=predictions, data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Global Scores:\")\n",
    "print(results.global_scores.summary)\n",
    "\n",
    "print(\"Instance Scores:\")\n",
    "print(results.instance_scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
