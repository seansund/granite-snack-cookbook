{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) intrinsics in Granite 3.2\n",
    "\n",
    "*Using IBM Granite Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the experimental intrinsics for Retrieval Augmented Generation (RAG) in Granite 3.2.\n",
    "\n",
    "RAG is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.\n",
    "\n",
    "The Granite 3.2 model's experimental intrinsics provides hallucination confidence and citation generation capabilities for RAG operations.\n",
    "\n",
    "RAG use cases include:\n",
    "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
    "- Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
    "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Initial setup:\n",
    "  - Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages using WatsonX, and store them in a vector database.\n",
    "- Upon each user query:\n",
    "  - Retrieve relevant passages from the database. In this recipe, we use an embedding of the query to retrieve semantically similar passages.\n",
    "  - Generate a response by feeding retrieved passages into a large language model, along with the user query.\n",
    "\n",
    "NOTE: In Granite 3.2, the hallucination confidence and citation generation capabilities are currently considered experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    \"transformers\" \\\n",
    "    \"accelerate\" \\\n",
    "    \"langchain_community\" \\\n",
    "    \"langchain_huggingface\" \\\n",
    "    \"langchain-milvus\" \\\n",
    "    \"replicate\" \\\n",
    "    \"docling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting System Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model to use for generating embedding vectors from text. Here we will be using one of the new [Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)\n",
    "\n",
    "To use a model from another provider, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the database to use for storing and retrieving embedding vectors.\n",
    "\n",
    "To connect to a vector database other than Milvus, replace this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "import tempfile\n",
    "\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\n",
    "print(f\"The vector database will be saved to {db_file}\")\n",
    "\n",
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Granite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, from a set of source documents, we use [Docling](https://docling-project.github.io/docling/) to convert the documents into text and then split the text into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Docling to download the documents, convert to text, and split into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use a set of web pages about IBM and the US Open. For each source web page, we convert the web page into a DoclingDocument and then chunk the DoclingDocument. Finally LangChain Documents are created for all the chunks labeled text or paragraph. The Documents are annotated with metadata to define a unique document id and the source of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "sources = [\n",
    "    \"https://www.ibm.com/case-studies/us-open\",\n",
    "    \"https://www.ibm.com/sports/usopen\",\n",
    "    \"https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement\",\n",
    "    \"https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms\",\n",
    "]\n",
    "\n",
    "converter = DocumentConverter()\n",
    "i = 0\n",
    "texts: list[Document] = [\n",
    "    Document(page_content=chunk.text, metadata={\"doc_id\": (i:=i+1), \"source\": source})\n",
    "    for source in sources\n",
    "    for chunk in HierarchicalChunker().chunk(converter.convert(source=source).document)\n",
    "    if any(filter(lambda c: c.label in [DocItemLabel.TEXT, DocItemLabel.PARAGRAPH], iter(chunk.meta.doc_items)))\n",
    "]\n",
    "\n",
    "print(f\"{len(texts)} documents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Populate the vector database\n",
    "\n",
    "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_db.add_documents(texts)\n",
    "print(f\"{len(ids)} documents added to the vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the query to use for the RAG operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the database for similar documents by proximity of the embedded vector in vector space to demonstrate the similarity search used during the RAG operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_db.similarity_search(query)\n",
    "print(f\"{len(docs)} documents returned\")\n",
    "for d in docs:\n",
    "    print(f\"doc_id={d.metadata['doc_id']}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the prompt for Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace. We also use Granite chat template controls to enable the hallucination confidence and citation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from string import Formatter\n",
    "\n",
    "# controls for enabling Granite intrinsic capability for hallucination confidence and citations\n",
    "controls = {\n",
    "    \"hallucinations\": True,\n",
    "    \"citations\": True,\n",
    "}\n",
    "\n",
    "# Create a Granite prompt for question-answering with the retrieved context\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "      }],\n",
    "    documents=[{\n",
    "        \"title\": \"placeholder\",\n",
    "        \"text\": \"{context}\",\n",
    "    }],\n",
    "    controls=controls,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# The Granite prompt can contain JSON strings\n",
    "def escape_f_string(f_string: str, *keys: str) -> str:\n",
    "    \"\"\"Escape non-keys in the specified f-string.\n",
    "\n",
    "    Args:\n",
    "        f_string (str): The f-string to escape.\n",
    "        keys: The key names which are part of the f-string and should not be escaped.\n",
    "\n",
    "    Returns:\n",
    "        str: The f-string with non-keys escaped in double braces.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for literal_text, field_name, format_spec, conversion in Formatter().parse(f_string):\n",
    "        if literal_text:\n",
    "            result.append(literal_text)\n",
    "        if field_name is not None:\n",
    "            is_key = field_name in keys\n",
    "            result.append(\"{\" if is_key else \"{{\")\n",
    "            result.append(field_name)\n",
    "            if conversion:\n",
    "                result.append(\"!\")\n",
    "                result.append(conversion)\n",
    "            if format_spec:\n",
    "                result.append(\":\")\n",
    "                result.append(format_spec)\n",
    "            result.append(\"}\" if is_key else \"}}\")\n",
    "    return \"\".join(result)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=escape_f_string(prompt, \"input\", \"context\"))\n",
    "\n",
    "# Create a Granite document prompt template to wrap each retrieved document\n",
    "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "Document {doc_id}\n",
    "{page_content}\"\"\")\n",
    "document_separator=\"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now build a RAG chain with the model and the document retriever and the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Assemble the retrieval-augmented generation chain.\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the RAG chain to process a question. The document chunks relevant to that question are retrieved and used as context. Since we enabled Granite hallucination confidence and citation capabilities, the response include information about this in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(output['answer'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
