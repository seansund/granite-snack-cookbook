{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "625bb31a-0fc9-4a52-a77d-020ead3b51fd"
   },
   "source": [
    "# Chain of thought (cot) reasoning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7ad82e54-69ec-4a61-8536-0617e1911406"
   },
   "source": [
    "As [large language models (LLMs)](https://www.ibm.com/think/topics/large-language-models) continue to revolutionize how we interact with artificial intelligence (AI), [prompt engineering](https://www.ibm.com/think/topics/prompt-engineering) has emerged as a critical skill. One of the most effective and widely discussed prompting techniques in the world of [generative AI](https://www.ibm.com/think/topics/generative-ai) is [chain ofthoughts](https://www.ibm.com/think/topics/chain-of-thoughts) prompting (also known as CoT prompting). This method improves a model’s ability to solve complex problems by encouraging it to generate intermediate reasoning steps, just like a human would when thinking through a problem.\n",
    "\n",
    "Unlike standard prompting, where [machine learning](https://www.ibm.com/think/topics/machine-learning) models are asked to provide direct answers, chain of thought prompting works by guiding the model to “think out loud” in natural language to reach a conclusion. This shift leads to significant gains in problem-solving accuracy across a variety of [benchmarks](https://www.ibm.com/think/topics/llm-benchmarks), especially in tasks that require multistep reasoning or logical inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cd6d0c67-67d6-4421-9a59-3e53e0a7921d"
   },
   "source": [
    "# Chain of thought reasoning in Granite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "655f5db3-7e31-4d01-bbcc-0952c06c2db8"
   },
   "source": [
    "This notebook demonstrates the use of chain of thought prompting to unlock the [reasoning](https://www.ibm.com/think/topics/ai-reasoning) capabilities of the IBM® [Granite® Instruct](https://www.ibm.com/granite) large language models. \n",
    "\n",
    "Unlike traditional [AI models](https://www.ibm.com/think/topics/ai-model), Instruct LLMs have common sense reasoning embedded in them directly through fine tuning, allowing them to perform complex reasoning tasks without relying on external modules. The Granite Instruct internal reasoning process can be toggled on or off (See Reasoning when you need it for additional information) to optimize compute usage depending on the reasoning tasks involved. This process makes it possible to observe the step-by-step reasoning path as Granite tackles complex tasks. This view reveals how it forms connections, processes natural language and arrives at the final answer, similar to watching an expert’s thought process unfold. Furthermore, the consistency of Granite improves chain of thought prompting by sampling multiple reasoning paths and selecting the most consistent answer, boosting reliability and accuracy.\n",
    "\n",
    "\n",
    "This tutorial will guide you through the fundamentals of CoT prompting with Granite Instruct models. We’ll also explore how different datasets, prompt engineering techniques and prompt engineering approaches affect performance, and why chainofthought reasoning often outperforms standard prompting on real-world benchmarks involving complex problems. Explore the [open source](https://www.ibm.com/think/topics/open-source#:~:text=Open%20source%20refers%20to%20software,collaboratively%20by%20a%20global%20community.) IBM Granite Community project powering this tutorial on [GitHub](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/Intrinsics/Granite_Reasoning.ipynb).\n",
    "\n",
    "NOTE: In Granite 3.2, the chain of thought reasoning capabilities are currently considered experimental."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a0a19964-fcae-4695-bd77-af6d6073f5fc"
   },
   "source": [
    "## Step 1. Install dependencies\n",
    "\n",
    "Next, install the Python package dependencies for this notebook. Granite utils provide some helpful functions for recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8cead79-fd6d-4065-ad89-3db2bf418a52"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
    "    langchain_community \\\n",
    "    transformers \\\n",
    "    replicate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9b262e59-44e5-4884-ab2a-50db70b0e529"
   },
   "source": [
    "## Step 2. Select your model\n",
    "\n",
    "Select a Granite model from the ibm-granite org on Replicate. While there is a smaller model, (Granite-3.2-2b-instruct) for the purpose of this tutorial Granite-3.2-8b-instruct is the default. It is important to note that model size plays a role in the ability to handle tasks such as logic and math without being explicitly trained to do so, also referred to as emergent reasoning. This ability tends to appear naturally as the models scale.\n",
    "\n",
    "Here we use the Replicate Langchain client to connect to the model.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e0a6bcb-d7fc-4cf5-ab0e-8b1cf14c92af"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_community.llms import Replicate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 4000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.0, # Lower the temperature\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ca95beff-f419-4dfb-bc03-dcb100c57191"
   },
   "source": [
    "## Step 3. Setup two prompts\n",
    "\n",
    "Next, create two prompt chains. The first chain will use the model’s normal (non-chain of thought reasoning) response mode. The normal response mode is the default prompt mode for Granite. The second chain is configured to use the chain of thought reasoning response mode. This step is done by passing ```thinking=True``` to the chat template. When doing so, it adds specific instructions to the system prompt, causing the model's internal reasoning process to be activated which results in the response containing the reasoning steps. By exploring variants of chain of thought prompting, one can experiment with how the models approach decision-making, making them more adaptable to a wide range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32be7388-6056-48ae-acd4-a927b646b03d"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a Granite prompt without chain-of-thought reasoning\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(template=prompt)\n",
    "chain = prompt_template | model\n",
    "\n",
    "# Create a Granite prompt using chain-of-thought reasoning\n",
    "reasoning_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    thinking=True, # Use chain-of-thought reasoning\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "reasoning_prompt_template = PromptTemplate.from_template(template=reasoning_prompt)\n",
    "reasoning_chain = reasoning_prompt_template | model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1de0fb4e-d01e-4dff-acba-18c7a73d878d"
   },
   "source": [
    "Now that the prompts have been created, take a look at the difference between them to see which activates the Granite model’s internal reasoning process in the reasoning prompt.\n",
    "\n",
    "NOTE: This additional prompt text is specific to the chat template for the version of Granite used and can change in future releases of Granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb209e6d-e6de-4a8d-ab39-54046c369c06"
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "print(\"==== System prompt instructions for chain-of-thought reasoning ====\")\n",
    "diff = difflib.ndiff(prompt, reasoning_prompt)\n",
    "print(wrap_text(\"\".join(d[-1] for d in diff if d[0] == \"+\"), indent=\"  \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ba5757ed-77b0-448a-81e2-25e19353b28d"
   },
   "source": [
    "## Step 4. Compare the responses of the two prompts\n",
    "\n",
    "First, we define a helper function to take a question and use both prompts to respond to the question. The function will display the question and then display the response from the normal prompt, without CoT followed by the step-by-step response from the chain of thought reasoning prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d72e32c2-9ecb-4e49-9b42-cee2929b4b26"
   },
   "outputs": [],
   "source": [
    "def question(question: str) -> None:\n",
    "    print(\"==== Question ====\")\n",
    "    print(wrap_text(question, indent=\"  \"))\n",
    "\n",
    "    print(\"==== Normal prompt response ====\")\n",
    "    output = chain.invoke({\"input\": question})\n",
    "    print(wrap_text(output, indent=\"  \"))\n",
    "\n",
    "    print(\"\\n==== Reasoning prompt response ====\")\n",
    "    reasoning_output = reasoning_chain.invoke({\"input\": question})\n",
    "    print(wrap_text(reasoning_output, indent=\"  \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "86db3418-02ae-4ead-b244-548f316aba01"
   },
   "source": [
    "## Step 5. Chain of thoughts reasoning use cases\n",
    "\n",
    "In this example, chain of thought prompting supports logical problem-solving by having the model summarize the given relationships before analyzing them in detail. This helps ensure that each part of the problem is clearly understood and leads to an accurate conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb251645-ca9a-43bc-9da1-c32c5051b03a"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Sally is a girl and has 3 brothers.\n",
    "Each brother has 2 sisters.\n",
    "How many sisters does Sally have?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "91853d0b-5ba7-4e2e-a177-465227e234ed"
   },
   "source": [
    "The following example demonstrates how chain of thought prompting helps large language models handle basic decision-making and comparison-based problem-solving. This capacity makes the model's reasoning abilities and reasoning paths more transparent and accurate, turning a simple question into a short exercise in decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd69ee77-01a3-4d94-bd71-dcdc66db811d"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which of the following items weigh more: a pound of water, two pounds of bricks, a pound of feathers, or three pounds of air?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a5e3e8ed-0c8f-4d84-97bd-1786a1c2bf10"
   },
   "source": [
    "This next example highlights how chain of thought prompting allows large language models to work through basic numerical comparisons with greater clarity. By encouraging step-by-step reasoning, even simple math-based questions become transparent exercises in evaluating magnitude and numerical relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38b2ce05-b27c-4ae2-a63e-92194f20a988"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which one is greater, 9.11 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7c36e4f1-2b5f-4b2a-b807-c98d1b3da9fc"
   },
   "source": [
    "Building on the previous example of comparing decimal numbers, this question explores how the context of versioning can change the interpretation of similar-looking values. Chain of thought prompting helps clarify the subtle difference between numerical and version-based comparisons, guiding the model to apply reasoning that's sensitive to real-world conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab493aee-cfd0-450f-8ab2-246ff88b405b"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which version number is greater, 9.11 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "85a8c6a0-5303-47a6-9db5-3e5a9fc5df5f"
   },
   "source": [
    "Continuing the exploration of version comparisons, this example introduces Maven versioning and the impact of prerelease identifiers such as -rc1 (release candidate). Chain of thought prompting allows the model to navigate domain-specific rules—such as semantic version precedence—making it easier to reason about which version is considered \"greater\" in practical software versioning contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80d26511-7ba3-4d3f-bd82-53a156f18ce5"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which Maven version number is greater, 9.9-rc1 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9df493-c31c-41c2-b211-c002ee3dcba9"
   },
   "source": [
    "Chain of thought prompting helps models solve math word problems by breaking them down into clear, step-by-step reasoning. Instead of jumping to the final answer, the model explains how quantities and percentages relate, mimicking the logical reasoning of how a student might work through a mixture problem logically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0454c25-7784-4af3-a4be-b18dcd23bbd0"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "You have 10 liters of a 30% acid solution.\n",
    "How many liters of a 70% acid solution must be added to achieve a 50% acid mixture?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ca3e064c-051c-4370-bc80-7a0a434a81ab"
   },
   "source": [
    "The final example demonstrates how chain of thought prompting can support geometric reasoning by breaking down shape properties and applying fundamental rules, such as angle sums in triangles. It shows how a model can translate a brief problem statement into a structured logical process, leading to a clear and correct conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6de90b1-c880-42db-939a-5c74da74118b"
   },
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "In an isosceles triangle, the vertex angle measures 40 degrees.\n",
    "What is the measure of each base angle?\\\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8ba30d29-0aea-4afd-9b47-94ca94f9f755"
   },
   "source": [
    "## References \n",
    "\n",
    "Boshi Wang, S. M. (2022). Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. 2717-2739, https://doi.org/10.48550/arXiv.2212.10001.\n",
    "\n",
    "“IBM Granite 3.2 Documentation – IBM Granite.” 2024. Ibm.com. 2024. https://www.ibm.com/granite/docs/models/granite/.\n",
    "\n",
    "Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ArXiv:2203.11171 [Cs], October. https://arxiv.org/abs/2203.11171.\n",
    "\n",
    "“IBM Granite 3.2 Documentation – IBM Granite.” 2024. Ibm.com. 2024. https://www.ibm.com/granite/docs/models/granite/.\n",
    "\n",
    "‌Tian, Jacob-Junqi, Omkar Dige, D Emerson, and Faiza Khattak. 2023. “Using Chain-of-Thought Prompting for Interpretable Recognition of Social Bias.” OpenReview. 2023. https://openreview.net/forum?id=QyRganPqPz&referrer=%5Bthe%20profile%20of%20D.%20B.%20Emerson%5D(%2Fprofile%3Fid%3D~D._B._Emerson1).\n",
    "\n",
    "‌Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi Quoc, V Le, and Denny Zhou. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Chain-of-Thought Prompting.” https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
