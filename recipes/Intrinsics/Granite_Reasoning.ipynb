{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Chain-of-thought Reasoning in Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the chain-of-thought reasoning capabilities of the Granite Instruct models.\n",
    "\n",
    "The reasoning capability is baked into the Instruct models rather then being a separate model. The modelâ€™s internal reasoning process can be easily toggled on and off, ensuring the appropriate use of compute resources for the task at hand. See [Reasoning when you need it](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision#Granite+3.2+Instruct%3A+Reasoning+when+you+need+it) for additional information.\n",
    "\n",
    "NOTE: In Granite 3.2, the chain-of-thought reasoning capabilities are currently considered experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "We first need to install some Python package dependencies for this notebook. Granite utils provides some helpful functions for recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
    "    langchain_community \\\n",
    "    transformers \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model\n",
    "\n",
    "Select a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_community.llms import Replicate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 4000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.0, # Lower the temperature\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Setup two prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two prompt chains. The first chain will use the normal (non-chain-of-thought reasoning) response mode. This is the default prompt mode for Granite. The second chain is configured to use the chain-of-thought reasoning response mode by passing `thinking=True` to the chat template. This adds specific instructions to the system prompt to cause the model's internal reasoning process to be activated which results in the response containing the chain-of-thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a Granite prompt without chain-of-thought reasoning\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(template=prompt)\n",
    "chain = prompt_template | model\n",
    "\n",
    "# Create a Granite prompt using chain-of-thought reasoning\n",
    "reasoning_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    thinking=True, # Use chain-of-thought reasoning\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "reasoning_prompt_template = PromptTemplate.from_template(template=reasoning_prompt)\n",
    "reasoning_chain = reasoning_prompt_template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the two prompts. Lets looks at the difference between them which activates the Granite model's internal reasoning process in the reasoning prompt.\n",
    "\n",
    "NOTE: This additional prompt text is specific to the chat template for the version of Granite used and may change in future releases of Granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "print(\"==== System prompt instructions for chain-of-thought reasoning ====\")\n",
    "diff = difflib.ndiff(prompt, reasoning_prompt)\n",
    "print(wrap_text(\"\".join(d[-1] for d in diff if d[0] == \"+\"), indent=\"  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the responses of the two prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a helper function to take a question and use both prompts to respond to the question. The function will display the question and then the response from the normal prompt followed by the response from the chain-of-thought reasoning prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(question: str) -> None:\n",
    "    print(\"==== Question ====\")\n",
    "    print(wrap_text(question, indent=\"  \"))\n",
    "\n",
    "    print(\"==== Normal prompt response ====\")\n",
    "    output = chain.invoke({\"input\": question})\n",
    "    print(wrap_text(output, indent=\"  \"))\n",
    "\n",
    "    print(\"\\n==== Reasoning prompt response ====\")\n",
    "    reasoning_output = reasoning_chain.invoke({\"input\": question})\n",
    "    print(wrap_text(reasoning_output, indent=\"  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells each contain an interesting questions for the model to answer. Sometimes the normal response mode will not properly answer the question while the chain-of-thought reasoning response mode will reason out the correct answer.\n",
    "\n",
    "Try the following questions. You can edit/add a cell to try you own question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Sally is a girl and has 3 brothers.\n",
    "Each brother has 2 sisters.\n",
    "How many sisters does Sally have?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which of the following items weigh more: a pound of water, two pounds of bricks, a pound of feathers, or three pounds of air?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which one is greater, 9.11 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which version number is greater, 9.11 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "Which Maven version number is greater, 9.9-rc1 or 9.9?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "You have 10 liters of a 30% acid solution.\n",
    "How many liters of a 70% acid solution must be added to achieve a 50% acid mixture?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question(\"\"\"\\\n",
    "In an isosceles triangle, the vertex angle measures 40 degrees.\n",
    "What is the measure of each base angle?\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now seen how to use the chain-of-though reasoning capability in the Granite Instruct models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
