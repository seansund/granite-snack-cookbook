{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Fine-tuned Model  \n",
        "  \n",
        "We now demonstrate how to load a model back for inference. This step is crucial for real-world applications where you want to use your trained model without going through the training process again.  \n",
        "  \n",
        "Loading a saved model is typically much faster than training from scratch, making it efficient for deployment scenarios. We'll show how to load both the model and the tokenizer, ensuring that we have all the components necessary for text generation.  \n"
      ],
      "metadata": {
        "id": "FpdG2lh-Sr_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yusdD7EHSicY"
      },
      "outputs": [],
      "source": [
        "!pip install \"transformers>=4.45.2\" peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model from Huggingface Hub  \n",
        "  \n",
        "Once a model is pushed to the Hugging Face Hub, loading it for inference or further fine-tuning becomes remarkably straightforward. This ease of use is one of the key advantages of the Hugging Face ecosystem.  \n",
        "  \n",
        "We'll show how to load our fine-tuned model directly from the Hugging Face Hub using just a few lines of code. This process works not only for our own uploaded models but for any public model on the Hub, demonstrating the power and flexibility of this approach.  \n",
        "  \n",
        "Loading from the Hub allows you to:  \n",
        "1. Quickly experiment with different models  \n",
        "2. Easily integrate state-of-the-art models into your projects  \n",
        "3. Ensure you're using the latest version of a model  \n",
        "4. Access models from various devices or environments without needing to manually transfer files  \n",
        "  \n",
        "This capability is particularly useful in production environments, where you might need to dynamically load or update models based on specific requirements or performance metrics.  "
      ],
      "metadata": {
        "id": "tuDMRmoQTAxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model_name = \"ibm-granite/granite-3.0-2b-instruct\"\n",
        "adapter_model_name = \"rawkintrevo/granite-3.0-2b-instruct-pirate-adapter\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, adapter_model_name, device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
      ],
      "metadata": {
        "id": "2-h-R1vrSknZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation  \n",
        "  \n",
        "Just as we did in the fine tuning example, we'll evaluate our fine-tuned model by presenting it with the 'inheritance' prompt we used in the sanity check. This comparison will reveal how the model's output is now 'more piratey'.  \n",
        "  \n",
        "This step demonstrates the power of transfer learning and domain-specific fine-tuning in natural language processing, showing how we can adapt a general-purpose language model to specialized tasks."
      ],
      "metadata": {
        "id": "DXIIhWfPTHkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"<|user>What does 'inheritance' mean?\\n<|assistant|>\\n\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "stop_token = \"<|endoftext|>\"\n",
        "stop_token_id = tokenizer.encode(stop_token)[0]\n",
        "outputs = model.generate(**inputs, max_new_tokens=500, eos_token_id=stop_token_id)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "kBuzWbI4Tgvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}