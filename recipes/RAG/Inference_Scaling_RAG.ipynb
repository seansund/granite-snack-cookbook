{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh8090CYsCPi"
   },
   "source": [
    "# Improving Multimodal RAG with Inference Scaling\n",
    "*Using IBM Granite, Docling, and Langchain*\n",
    "\n",
    "Welcome to this Granite recipe. In this recipe, you'll learn how to harness the power of inference scaling to improve the multimodal RAG pipeline created in a [previous recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/Granite_Multimodal_RAG.ipynb).\n",
    "\n",
    "## Introduction to Inference Scaling\n",
    "\n",
    "Inference scaling is a technique that allocates more computational resources during inference time to achieve better performance. Unlike training-time scaling (using larger models), inference scaling leverages additional computation when generating responses without changing the underlying model. This approach can lead to almost linear improvements in performance when properly implemented.\n",
    "\n",
    "This tutorial will guide you through the following processes:\n",
    "\n",
    "- **Document preprocessing:** You will learn how to handle documents from various sources, parse and transform them into usable formats and store them in vector databases by using Docling. You will use a Granite MLLM to generate image descriptions of images in the documents.\n",
    "- **RAG:** Understand how to connect LLMs such as Granite with external knowledge bases to enhance query responses and generate valuable insights.\n",
    "- **Implementing Demonstration-based RAG (DRAG) and Iterative Demonstration-based RAG (IterDRAG):** Apply the inference scaling techniques from the research paper to significantly improve RAG performance when working with long context.\n",
    "- **LangChain for workflow integration:** Discover how to use LangChain to streamline and orchestrate document processing and retrieval workflows, enabling seamless interaction between different components of the system.\n",
    "\n",
    "And uses three cutting-edge technologies:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
    "2. **[Granite](https://www.ibm.com/granite/docs/models/granite/):** A state-of-the-art family of LLMs that provide robust natural language capabilities and a vision language model that provides image to text generation.\n",
    "3. **[LangChain](https://github.com/langchain-ai/langchain):** A powerful framework used to build applications powered by language models, designed to simplify complex workflows and integrate external tools seamlessly.\n",
    "\n",
    "By the end of this recipe, you will accomplish the following:\n",
    "- Gain proficiency in document preprocessing, chunking and image understanding.\n",
    "- Integrate vector databases to enhance retrieval capabilities.\n",
    "- Implement DRAG and IterDRAG to perform efficient and accurate data retrieval with inference scaling.\n",
    "- Experience firsthand how scaling inference compute can lead to almost linear improvements in RAG performance.\n",
    "\n",
    "This recipe is designed for AI developers, researchers and enthusiasts looking to enhance their knowledge of document management and advanced natural language processing (NLP) techniques.\n",
    "\n",
    "## Understanding Long-Context Challenges\n",
    "\n",
    "Traditional language models struggle with long contexts for several reasons:\n",
    "- Attention mechanisms scale quadratically with input length\n",
    "- Difficulty in locating relevant information in very long sequences\n",
    "- Challenges in preserving coherence across distant parts of the input\n",
    "- Increased computational demands for processing long sequences\n",
    "\n",
    "The techniques in this tutorial address these challenges through strategic allocation of inference computation.\n",
    "\n",
    "## Inference Scaling Methods: DRAG and IterDRAG\n",
    "\n",
    "![IterDRAG breaks down the input query into sub-queries and answer them to improve the accuracy of the final answer. At inference time, IterDRAG scales the computation through multiple inference steps to decompose complex queries and retrieve documents.](https://arxiv.org/html/2410.04343v2/x1.png \"DRAG vs. IterDRAG\")\n",
    "\n",
    "*DRAG vs IterDRAG Diagram* from [\"Inference Scaling for Long-Context Retrieval Augmented Generation\"](https://arxiv.org/abs/2410.04343)\n",
    "\n",
    "This notebook implements two advanced inference scaling techniques from the research paper [\"Inference Scaling for Long-Context Retrieval Augmented Generation\"](https://arxiv.org/abs/2410.04343):\n",
    "\n",
    "1. **DRAG (Demonstration-based RAG)**: This method leverages in-context learning to improve RAG performance. By including multiple RAG examples as demonstrations, DRAG helps models learn to locate relevant information in long contexts. Unlike standard RAG which might plateau with more documents, DRAG shows linear improvements with increased context length.\n",
    "\n",
    "2. **IterDRAG (Iterative Demonstration-based RAG)**: An extension of DRAG that addresses complex multi-hop queries by decomposing them into simpler sub-queries. IterDRAG interleaves retrieval and generation steps, creating reasoning chains that bridge compositional gaps. This approach is particularly effective for handling complex queries across long contexts.\n",
    "\n",
    "These methods show that scaling inference computation can improve RAG performance almost linearly when optimally allocated, allowing RAG systems to make better use of long-context capabilities of modern LLMs.\n",
    "\n",
    "For this implementation, we'll use an IBM Granite model capable of processing different modalities. You'll create an AI system to answer real-time user queries from unstructured data, applying the principles from the paper.\n",
    "\n",
    "[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is a technique used with large language models (LLMs) to connect the model with a knowledge base of information outside the data the LLM has been trained on without having to perform [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning). Traditional RAG is limited to text-based use cases such as text summarization and chatbots.\n",
    "\n",
    "Multimodal RAG can use [multimodal](https://www.ibm.com/think/topics/multimodal-ai) LLMs (MLLM) to process information from multiple types of data to be included as part of the external knowledge base used in RAG. Multimodal data can include text, images, audio, video or other forms. In this recipe we use IBM's latest multimodal vision model, Granite 3.2 vision.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Familiarity with Python programming.\n",
    "- Basic understanding of LLMs, NLP concepts and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN2mK175_JRH",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfg8kTVr_JRH"
   },
   "source": [
    "Ensure you are running Python 3.10, 3.11 or 3.12 in a freshly created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoM938B_JRH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p_2cX1-_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfMWUUSs_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f394ded8-32e1-44da-8742-6cdab06408b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    langchain_huggingface \\\n",
    "    langchain_milvus \\\n",
    "    docling \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65eDG9U3sCPk"
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsyITy60sCPk"
   },
   "source": [
    "To see some logging information, we can configure INFO log level.\n",
    "\n",
    "NOTE: It is okay to skip running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zpBLqg_sCPk"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gu-Oeay_JRJ"
   },
   "source": [
    "## Step 3: Selecting the AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Piwy8BABsCPk"
   },
   "source": [
    "### Load the Granite models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFuZBhG-_JRJ"
   },
   "source": [
    "Specify the embeddings model to use for generating text embedding vectors. Here we will use one of the [Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)\n",
    "\n",
    "To use a different embeddings model, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvztNZly_JRJ",
    "outputId": "134502e8-7022-42c1-dc3d-a5538d8b0e6c"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW-axzJAsCPk"
   },
   "source": [
    "Specify the MLLM to use for image understanding. We will use the Granite vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGhwKYh-sCPk",
    "outputId": "5e79e512-23d7-4696-e31d-98dff4811dd8"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_community.llms import Replicate\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "vision_model_path = \"ibm-granite/granite-vision-3.2-2b\"\n",
    "vision_model = Replicate(\n",
    "    model=vision_model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.01,\n",
    "    },\n",
    ")\n",
    "vision_processor = AutoProcessor.from_pretrained(vision_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma8eWR10_JRJ"
   },
   "source": [
    "Specify the language model to use for the RAG generation operation. Here we use the Replicate LangChain client to connect to a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ckyj7Zrh_JRK"
   },
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.01\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviHG3n7_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4: Preparing the documents for the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ7Guu7A_JRK"
   },
   "source": [
    "In this example, from a set of source documents, we use [Docling](https://docling-project.github.io/docling/) to convert the documents into text and images. The text is then split into chunks. The images are processed by the MLLM to generate image summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuB8kkzf_JRK"
   },
   "source": [
    "### Use Docling to download the documents and convert to text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se6So2yw_JRK"
   },
   "source": [
    "Docling will download the PDF documents and process them so we can obtain the text and images the documents contain. In the PDF, there are various data types, including text, tables, graphs and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNGz_0gZ_JRK"
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pdf_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=False,\n",
    "    generate_picture_images=True,\n",
    ")\n",
    "format_options = {\n",
    "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
    "}\n",
    "converter = DocumentConverter(format_options=format_options)\n",
    "\n",
    "sources = [\n",
    "    \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\",\n",
    "]\n",
    "conversions = { source: converter.convert(source=source).document for source in sources }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVBYwCwVsCPk"
   },
   "source": [
    "With the documents processed, we then further process the text elements in the documents. We chunk them into appropriate sizes for the embeddings model we are using. A list of LangChain documents are created from the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZ1cIA5DsCPk",
    "outputId": "236a542c-2c7e-431f-868c-a1fa47d488a3"
   },
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "from docling_core.types.doc import DocItem, TableItem\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc_id = 0\n",
    "texts: list[Document] = []\n",
    "for source, docling_document in conversions.items():\n",
    "    for chunk in HybridChunker(tokenizer=embeddings_tokenizer).chunk(docling_document):\n",
    "        items: list[DocItem] = chunk.meta.doc_items # type: ignore\n",
    "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
    "            continue # we will process tables later\n",
    "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
    "        print(refs)\n",
    "        text = chunk.text\n",
    "        document = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"doc_id\": (doc_id:=doc_id+1),\n",
    "                \"source\": source,\n",
    "                \"ref\": refs,\n",
    "            },\n",
    "        )\n",
    "        texts.append(document)\n",
    "\n",
    "print(f\"{len(texts)} text document chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wje3H-q6sCPl"
   },
   "source": [
    "Next we process any tables in the documents. We convert the table data to markdown format for passing into the language model. A list of LangChain documents are created from the table's markdown renderings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJsFQM1psCPl",
    "outputId": "2b7cd5e4-2a1b-4323-9aa8-a1fb913e5e26"
   },
   "outputs": [],
   "source": [
    "from docling_core.types.doc import DocItemLabel\n",
    "\n",
    "doc_id = len(texts)\n",
    "tables: list[Document] = []\n",
    "for source, docling_document in conversions.items():\n",
    "    for table in docling_document.tables:\n",
    "        if table.label in [DocItemLabel.TABLE]:\n",
    "            ref = table.get_ref().cref\n",
    "            print(ref)\n",
    "            text = table.export_to_markdown(docling_document)\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref\n",
    "                },\n",
    "            )\n",
    "            tables.append(document)\n",
    "\n",
    "\n",
    "print(f\"{len(tables)} table documents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz5utAL1sCPl"
   },
   "source": [
    "Finally we process any images in the documents. Here we use the vision language model to understand the content of an image. In this example, we are interested in any textual information in the image.\n",
    "\n",
    "The choice of image prompt is critical as it directs what aspects of the image the model will focus on. For example:\n",
    "- A prompt like \"Give a detailed description of what is depicted in the image\" (used below) will provide general information about all visual elements\n",
    "- A prompt like \"What text appears in this image?\" would focus specifically on extracting textual content\n",
    "- A prompt like \"Describe the graphical data visualization in this image\" would be better for charts and graphs\n",
    "\n",
    "You should experiment with different prompts based on the types of images in your documents and what information you need to extract from them.\n",
    "\n",
    "NOTE: Processing the images may require significant processing time depending upon the number of images and the service running the vision language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82Z5blH1sCPl",
    "outputId": "b3e54159-9638-4ba8-eaf3-1f234255f8fb"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri\n",
    "\n",
    "# Feel free to experiment with this prompt\n",
    "image_prompt = \"Give a detailed description of what is depicted in the image\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": image_prompt},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "vision_prompt = vision_processor.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "pictures: list[Document] = []\n",
    "doc_id = len(texts) + len(tables)\n",
    "for source, docling_document in conversions.items():\n",
    "    for picture in docling_document.pictures:\n",
    "        ref = picture.get_ref().cref\n",
    "        print(ref)\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                },\n",
    "            )\n",
    "            pictures.append(document)\n",
    "\n",
    "print(f\"{len(pictures)} image descriptions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdyu_TDEsCPl"
   },
   "source": [
    "We can then display the LangChain documents created from the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "htIYVVjHPKSX",
    "outputId": "23b8dae8-e93f-4c67-b3f5-7bad503f4d99"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from docling_core.types.doc import RefItem\n",
    "from IPython.display import display\n",
    "\n",
    "# Print all created documents\n",
    "for document in itertools.chain(texts, tables):\n",
    "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
    "    print(f\"Source: {document.metadata['source']}\")\n",
    "    print(f\"Content:\\n{document.page_content}\")\n",
    "    print(\"=\" * 80)  # Separator for clarity\n",
    "\n",
    "for document in pictures:\n",
    "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
    "    source = document.metadata['source']\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Content:\\n{document.page_content}\")\n",
    "    docling_document = conversions[source]\n",
    "    ref = document.metadata['ref']\n",
    "    picture = RefItem(cref=ref).resolve(docling_document)\n",
    "    image = picture.get_image(docling_document)\n",
    "    print(\"Image:\")\n",
    "    display(image)\n",
    "    print(\"=\" * 80)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bjz1IR3_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Populate the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKT1ZZoysCPl"
   },
   "source": [
    "Using the embedding model, we load the documents from the text chunks and generated image captioning into a vector database. Creating this vector database allows us to easily conduct a semantic similarity search across our documents.\n",
    "\n",
    "NOTE: Population of the vector database may require significant processing time depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNjFYpNrsCPl"
   },
   "source": [
    "### Choose your vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe7g4WoasCPl"
   },
   "source": [
    "Specify the database to use for storing and retrieving embedding vectors.\n",
    "\n",
    "To connect to a vector database other than Milvus, replace this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "0nq9W6M_sCPo",
    "outputId": "7377a576-b5b5-405c-d369-901c3e61d44c"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
    "print(f\"The vector database will be saved to {db_file}\")\n",
    "\n",
    "vector_db: VectorStore = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT28uYDIsCPo"
   },
   "source": [
    "We now add all the LangChain documents for the text, tables and image descriptions to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "documents = list(itertools.chain(texts, tables, pictures))\n",
    "ids = vector_db.add_documents(documents)\n",
    "print(f\"{len(ids)} documents added to the vector database\")\n",
    "retriever: VectorStoreRetriever = vector_db.as_retriever(search_kwargs={\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq50gMAO_JRK"
   },
   "source": [
    "## Step 5: RAG with Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZOYJW0D_JRL"
   },
   "source": [
    "Now that we have successfully converted our documents and vectorized them, we can set up our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf8T8eZk_JRL"
   },
   "source": [
    "### Validate Retrieval Quality\n",
    "\n",
    "Here we test the vector database by searching for chunks with relevant information to our query in the vector space. We display the documents associated with the retrieved image description.\n",
    "\n",
    "This validation step is important to ensure that our retrieval system is working correctly before we build our full RAG pipeline. We want to see if the returned documents are actually relevant to our query.\n",
    "\n",
    "Feel free to try different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMtTSHhQ_JRL"
   },
   "outputs": [],
   "source": [
    "query = \"Analyze how Midwest Food Bank's financial efficiency changed during the pandemic by comparing their 2019 and 2020 performance metrics. What specific pandemic adaptations had the greatest impact on their operational capacity, and how did their volunteer management strategy evolve to maintain service levels despite COVID-19 restrictions? Provide specific statistics from the report to support your analysis.\"\n",
    "for doc in vector_db.as_retriever().invoke(query):\n",
    "    print(doc)\n",
    "    print(\"=\" * 80)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viFYsnxTS2OC"
   },
   "source": [
    "The returned documents should be responsive to the query. Let's go ahead and construct our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxVuFY_A_JRL"
   },
   "source": [
    "### Create the RAG pipeline for Granite\n",
    "\n",
    "First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.\n",
    "\n",
    "`{context}` will hold the retrieved chunks, as shown in the previous search, and feeds this to the model as document context for answering our question.\n",
    "\n",
    "Then, we construct the RAG pipeline by using the Granite prompt templates we created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB-CPPTo_JRL"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import escape_f_string\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create a Granite prompt for question-answering with the retrieved context\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"0\",\n",
    "        \"text\": \"{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(template=escape_f_string(prompt, \"input\", \"context\"))\n",
    "\n",
    "# Create a Granite document prompt template to wrap each retrieved document\n",
    "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
    "{page_content}\"\"\")\n",
    "document_separator=\"\"\n",
    "\n",
    "# Assemble the retrieval-augmented generation chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_NU_Yhl_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXQzDqAB_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pipeline uses the query to locate documents from the vector database and use them as context for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdo9PXsU_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = rag_chain.invoke({\"input\": query})\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_uyUdCu_JRQ"
   },
   "source": [
    "## Standard RAG Limitations and Why We Need Inference Scaling\n",
    "\n",
    "While the standard RAG approach works reasonably well, it has several key limitations when dealing with long or complex content:\n",
    "\n",
    "1. **Context Management**: When dealing with many documents, standard RAG struggles to effectively utilize all the available context.\n",
    "2. **Retrieval Quality**: Without guidance on how to use the retrieved information, models often focus on the wrong parts of documents.\n",
    "3. **Compositional Reasoning**: Complex queries requiring multi-step reasoning are challenging for standard RAG.\n",
    "4. **Performance Plateaus**: Adding more documents to standard RAG often results in diminishing returns after a certain threshold.\n",
    "\n",
    "Inference scaling techniques address these limitations by strategically allocating more computation at inference time.\n",
    "\n",
    "## Enhanced RAG with DRAG (Demonstration-based RAG)\n",
    "\n",
    "Now we'll implement the DRAG technique from the research paper [\"Inference Scaling for Long-Context Retrieval Augmented Generation\"](https://arxiv.org/abs/2410.04343) to enhance our RAG system.\n",
    "\n",
    "DRAG uses in-context examples to demonstrate to the model how to extract and use information from documents, improving performance for long-context scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8cm8_F1ErGP"
   },
   "source": [
    "### Step 1: Create sample in-context demonstrations\n",
    "\n",
    "These would typically come from a curated dataset of high-quality QA pairs.\n",
    "For this example, we'll create some synthetic examples that match the expected domain.\n",
    "\n",
    "We define a data class to represent an individual demonstration and then create some demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIBeRyrts9R4"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, InitVar\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@dataclass\n",
    "class DRAG_Demonstration:\n",
    "    query: str\n",
    "    answer: str\n",
    "    retriever: InitVar[VectorStoreRetriever] = field(kw_only=True)\n",
    "    documents: list[Document] = field(default_factory=list, kw_only=True)\n",
    "\n",
    "    def __post_init__(self, retriever: VectorStoreRetriever):\n",
    "        if not self.documents:\n",
    "            self.documents = retriever.invoke(self.query)\n",
    "\n",
    "    def __format__(self, format_spec: str) -> str:\n",
    "        formatted_documents = \"\\n\".join(\n",
    "            f\"Document {i+1}:\\n{document.page_content}\"\n",
    "            for i, document in enumerate(self.documents)\n",
    "        )\n",
    "        return f\"\"\"\\\n",
    "{formatted_documents}\n",
    "Question: {self.query}\n",
    "Answer: {self.answer}\n",
    "\"\"\"\n",
    "\n",
    "def create_enhanced_drag_demonstrations(vector_db: VectorStore) -> list[DRAG_Demonstration]:\n",
    "    \"\"\"Create high-quality demonstrations for DRAG technique that showcase effective document analysis\"\"\"\n",
    "    demonstration_retriever: VectorStoreRetriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    demonstrations = [\n",
    "        DRAG_Demonstration(\n",
    "            query=\"How did the COVID-19 pandemic impact Midwest Food Bank's operations in 2020?\",\n",
    "            answer=\"The COVID-19 pandemic significantly impacted Midwest Food Bank's operations in 2020. Despite challenges, MFB remained open and responsive to increased needs. They implemented safety protocols, reduced volunteer numbers for social distancing, and altered their distribution model to allow partner agencies to receive food safely. The pandemic created unprecedented food insecurity, with many people seeking assistance for the first time. MFB distributed 37% more food than in 2019, with a record 179 semi-loads of Disaster Relief family food boxes sent nationwide. The organization also faced supply chain disruptions and food procurement challenges in the early months but continued to find and distribute food. Community, business, and donor support helped fund operations and food purchases. Additionally, MFB began participating in the USDA Farmers to Families Food Box program in May 2020, distributing over $52 million worth of nutritious produce, protein, and dairy products.\",\n",
    "            retriever=demonstration_retriever\n",
    "        ),\n",
    "        DRAG_Demonstration(\n",
    "            query=\"What role did volunteers play at Midwest Food Bank during 2020, and how were they affected by the pandemic?\",\n",
    "            answer=\"Volunteers were described as 'the life-blood of the organization' in the 2020 annual report. Despite the pandemic creating safety challenges, volunteers demonstrated courage and dedication by increasing their hours to meet growing needs. MFB implemented safety protocols at each location and limited volunteer group sizes to allow for social distancing. This created a challenge as food needs increased while fewer volunteers were available to help. To address this gap, multiple MFB locations received assistance from the National Guard, who filled vital volunteer positions driving trucks, operating forklifts, and helping with food distributions. In 2020, 17,930 individuals volunteered 300,898 hours of service, equivalent to 150 full-time employees. The volunteer-to-staff ratio was remarkable with 450 volunteers for every 1 paid MFB staff member, highlighting the volunteer-driven nature of the organization during the crisis.\",\n",
    "            retriever=demonstration_retriever\n",
    "        ),\n",
    "        DRAG_Demonstration(\n",
    "            query=\"How did Midwest Food Bank's international programs perform during 2020, particularly in Haiti and East Africa?\",\n",
    "            answer=\"In 2020, Midwest Food Bank's international operations in East Africa and Haiti faced unique challenges but continued to serve communities. In East Africa (operated as Kapu Africa), strict lockdowns led to mass hunger, especially in slum areas. Kapu Africa distributed 7.2 million Tender Mercies meals, working with partner ministries to share food in food-insecure slums. A notable outcome was a spiritual awakening among recipients, with many asking why they were receiving help. In Haiti, the pandemic added to existing challenges, closing airports, seaports, factories, and schools. MFB Haiti more than doubled its food shipments to Haiti, delivering over 160 tons of food relief, nearly three-quarters being Tender Mercies meals. As Haitian children primarily receive nourishment from school lunches, MFB Haiti distributed Tender Mercies through faith-based schools and also partnered with over 20 feeding centers serving approximately 1,100 children daily. Nearly 1 million Tender Mercies meals were distributed in Haiti during 2020.\",\n",
    "            retriever=demonstration_retriever\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSFxp2aAEmax"
   },
   "source": [
    "### Step 2: Format the demonstrations for inclusion in the prompt\n",
    "\n",
    "We then format all the demonstrations together for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hOKJVJwpl2q"
   },
   "outputs": [],
   "source": [
    "# Format all demonstrations together\n",
    "demonstrations = create_enhanced_drag_demonstrations(vector_db)\n",
    "\n",
    "formatted_demonstrations = \"\\n\\n\".join(\n",
    "    f\"Example {i+1}:\\n{demo}\"\n",
    "    for i, demo in enumerate(demonstrations)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VjaYCV8EhCB"
   },
   "source": [
    "### Step 3: Create the DRAG prompt template\n",
    "\n",
    "Then we create the DRAG prompt for the model which includes the formatted demonstration examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCHKDMvhp1IK"
   },
   "outputs": [],
   "source": [
    "drag_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\\\n",
    "Here are examples of effectively extracting information from documents to answer questions.\n",
    "\n",
    "{formatted_demonstrations}\n",
    "\n",
    "Follow these examples when answering the user's question:\n",
    "\n",
    "{{input}}\"\"\",\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"0\",\n",
    "        \"text\": \"Placeholder{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Convert to prompt template\n",
    "drag_prompt_template = PromptTemplate.from_template(template=escape_f_string(drag_prompt, \"input\", \"context\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG7ZDOboFUVy"
   },
   "source": [
    "### Step 4: Create a custom retriever that reorders documents\n",
    "\n",
    "Normally the retriever will return the documents in similarity order with the most similar document first. We define a reordering retriever to reorder the result to reverse the order so that most similar document is last and thus closer to the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKCktZX6abVU"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from langchain_core.retrievers import BaseRetriever, RetrieverInput, RetrieverOutput\n",
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "\n",
    "class ReorderingRetriever(BaseRetriever):\n",
    "    base_retriever: BaseRetriever\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: RetrieverInput, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: typing.Any\n",
    "    ) -> RetrieverOutput:\n",
    "        docs = self.base_retriever._get_relevant_documents(query, run_manager=run_manager, **kwargs)\n",
    "        return list(reversed(docs))  # Reverse the order so higher-ranked docs are closer to query in prompt\n",
    "\n",
    "reordering_retriever = ReorderingRetriever(base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogy765ACFZev"
   },
   "source": [
    "### Step 5: Create DRAG pipeline\n",
    "\n",
    "We create the pipeline for the DRAG query using the DRAG prompt template and the reordering retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__deMUohp6bJ"
   },
   "outputs": [],
   "source": [
    "drag_combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=drag_prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "\n",
    "drag_chain = create_retrieval_chain(\n",
    "    retriever=reordering_retriever,\n",
    "    combine_docs_chain=drag_combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MU0Gb-ZOFdPV"
   },
   "source": [
    "### Step 6: Generate a DRAG-enhanced response to a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yg43AXImf4H"
   },
   "outputs": [],
   "source": [
    "drag_outputs = drag_chain.invoke({\"input\": query})\n",
    "print(\"\\n=== DRAG-Enhanced Answer ===\")\n",
    "print(drag_outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YImI-AF7rLGk"
   },
   "source": [
    "Great, looks like we got some improvements in the answer by giving it some examples. Let's try an even more thorough RAG technique next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s23R6wdBzt64"
   },
   "source": [
    "## Implementing IterDRAG (Iterative Demonstration-based RAG)\n",
    "\n",
    "IterDRAG extends DRAG by decomposing complex queries into simpler sub-queries and performing interleaved retrieval. This is particularly effective for complex multi-hop questions that require integrating information from multiple sources or reasoning across several steps.\n",
    "\n",
    "Key benefits of the iterative approach:\n",
    "- Breaks down complex questions into manageable pieces\n",
    "- Retrieves more relevant information for each sub-question\n",
    "- Creates explicit reasoning chains\n",
    "- Enables tackling questions that would be challenging in a single step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4lVxi1FHByl"
   },
   "source": [
    "### Step 1: Create a query decomposition chain\n",
    "\n",
    "The decomposition step is critical - it takes a complex query and breaks it into simpler, more focused sub-queries that can be answered individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwFSrNakHE8f"
   },
   "outputs": [],
   "source": [
    "decompose_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\\\n",
    "You are a helpful assistant that breaks down complex questions into simpler sub-questions.\n",
    "For multi-part or complex questions, generate 1-3 sub-questions that would help answer the main question.\n",
    "\n",
    "Here are examples of how to decompose complex questions:\n",
    "{demonstrations}\n",
    "\n",
    "Follow the above examples when breaking down the user's question.\n",
    "If the following question is already simple enough, just respond with \"No follow-up needed.\"\n",
    "\n",
    "Otherwise, break down the following question into simpler sub-questions. Format your response as:\n",
    "Follow up: [sub-question]\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "decompose_prompt_template = PromptTemplate.from_template(template=escape_f_string(decompose_prompt, \"input\", \"demonstrations\"))\n",
    "decompose_chain = decompose_prompt_template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ry9BUgWVHPBS"
   },
   "source": [
    "### Step 2: Create a sub-query answering chain\n",
    "\n",
    "The sub-query answering component handles each individual sub-question by retrieving relevant documents and generating focused intermediate answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeqL0ZjRHSKX"
   },
   "outputs": [],
   "source": [
    "intermediate_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\\\n",
    "You are a helpful assistant that answers specific questions based on the provided documents.\n",
    "\n",
    "Focus only on the sub-question and provide a concise intermediate answer.\n",
    "Please answer the following sub-question based on the provided documents.\n",
    "Format your response as:\n",
    "Intermediate answer: [your concise answer to the sub-question]\n",
    "\n",
    "Sub-question: {input}\n",
    "\"\"\"\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"0\",\n",
    "        \"text\": \"Placeholder{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "intermediate_prompt_template = PromptTemplate.from_template(template=escape_f_string(intermediate_prompt, \"input\", \"context\"))\n",
    "intermediate_combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=intermediate_prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "intermediate_chain = create_retrieval_chain(\n",
    "    retriever=reordering_retriever,\n",
    "    combine_docs_chain=intermediate_combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR2BWbyNHVvs"
   },
   "source": [
    "### Step 3: Create a final answer generation chain\n",
    "\n",
    "The final answer generation component combines all the intermediate answers to produce a comprehensive response to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZbRz6MHHajC"
   },
   "outputs": [],
   "source": [
    "final_prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\\\n",
    "You are a helpful assistant that provides comprehensive answers to questions.\n",
    "Use the intermediate answers to sub-questions to formulate a complete final answer.\n",
    "Please provide a final answer to the main question based on the intermediate answers to sub-questions.\n",
    "Format your response as:\n",
    "So the final answer is: [your comprehensive answer to the main question]\n",
    "\n",
    "Main question: {input}\n",
    "\n",
    "Sub-questions and intermediate answers:\n",
    "{context}\"\"\"\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "final_prompt_template = PromptTemplate.from_template(template=escape_f_string(final_prompt, \"input\", \"context\"))\n",
    "final_chain = final_prompt_template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZeEDURDHdi4"
   },
   "source": [
    "### Step 4: Create example demonstrations for IterDRAG\n",
    "\n",
    "Creating effective demonstrations is crucial for IterDRAG's performance. These examples show the model how to:\n",
    "1. Break down complex questions into simpler sub-questions\n",
    "2. Generate relevant intermediate answers\n",
    "3. Combine these answers into a coherent final response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7TGXp43HhT-"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IterDRAG_Demonstration_Base:\n",
    "    query: str\n",
    "    answer: str\n",
    "\n",
    "@dataclass\n",
    "class IterDRAG_Demonstration(IterDRAG_Demonstration_Base):\n",
    "    intermediate: list[IterDRAG_Demonstration_Base]\n",
    "\n",
    "    def __format__(self, format_spec: str) -> str:\n",
    "        sub_questions=\"\\n\".join(\n",
    "            f\"Follow up: {sub.query}\"\n",
    "            for sub in self.intermediate\n",
    "        )\n",
    "\n",
    "        return f\"Question: {self.query}\\n{sub_questions}\"\n",
    "\n",
    "def create_iterdrag_demonstrations() -> list[IterDRAG_Demonstration]:\n",
    "    \"\"\"Create examples showing how to decompose and answer complex questions\"\"\"\n",
    "\n",
    "    demonstrations = [\n",
    "        IterDRAG_Demonstration(\n",
    "            query=\"What impact did the pandemic have on the food bank's operations and distribution?\",\n",
    "            answer=\"The pandemic had a profound impact on food bank operations and distribution. Distribution volume increased by 60% to over 100 million pounds of food in 2020. Operationally, the food bank faced supply chain disruptions, volunteer shortages, and safety protocol challenges. In response, they implemented contactless distribution, expanded mobile pantries, created emergency food boxes for vulnerable populations, and developed virtual nutrition education. Despite these challenges, they successfully scaled operations to meet the unprecedented community need during the crisis.\",\n",
    "            intermediate=[\n",
    "                IterDRAG_Demonstration_Base(\n",
    "                    query=\"How did food distribution volume change during the pandemic?\",\n",
    "                    answer=\"Food distribution volume increased by 60% during the pandemic, rising from approximately 62 million pounds in 2019 to over 100 million pounds in 2020.\",\n",
    "                ),\n",
    "                IterDRAG_Demonstration_Base(\n",
    "                    query=\"What operational challenges did the food bank face during the pandemic?\",\n",
    "                    answer=\"The food bank faced challenges including supply chain disruptions, volunteer shortages due to social distancing requirements, and the need to implement new safety protocols for food handling and distribution.\",\n",
    "                ),\n",
    "                IterDRAG_Demonstration_Base(\n",
    "                    query=\"What new programs were implemented in response to the pandemic?\",\n",
    "                    answer=\"New programs included contactless distribution methods, expanded mobile pantry operations, emergency food boxes for vulnerable populations, and virtual nutrition education classes.\",\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        IterDRAG_Demonstration(\n",
    "            query=\"How does the food bank's financial management compare to industry standards for non-profits?\",\n",
    "            answer=\"The food bank demonstrates excellent financial management compared to industry standards. With 94% of its budget allocated to program services and only 6% to administrative and fundraising costs, it exceeds the industry benchmark of 85-90% for program spending. This financial efficiency places the food bank among the top-performing non-profits in terms of maximizing donor impact and minimizing overhead expenses.\",\n",
    "            intermediate=[\n",
    "                IterDRAG_Demonstration_Base(\n",
    "                    query=\"What percentage of the food bank's budget goes to program services versus administrative costs?\",\n",
    "                    answer=\"94% of the food bank's budget goes directly to program services, with only 6% allocated to administrative and fundraising costs.\",\n",
    "                ),\n",
    "                IterDRAG_Demonstration_Base(\n",
    "                    query=\"What are the industry standards for program spending versus overhead for food banks?\",\n",
    "                    answer=\"Industry standards suggest that well-run food banks typically allocate 85-90% of their budget to program services, with 10-15% for administrative and fundraising expenses.\",\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    return demonstrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDnvF2tkHrEI"
   },
   "source": [
    "### Step 5: Implement the IterDRAG function\n",
    "\n",
    "This function orchestrates the entire iterative process:\n",
    "1. Decompose the main question into sub-questions\n",
    "2. For each sub-question, retrieve relevant documents and generate an intermediate answer\n",
    "3. Combine all intermediate answers to produce the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQYyEg8yzuuV"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def iterative_drag(main_question: str) -> dict[str, typing.Any]:\n",
    "    \"\"\"\n",
    "    Implements IterDRAG: decomposing queries, retrieving documents for sub-queries,\n",
    "    and generating a final answer based on intermediate answers.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with IterDRAG: '{main_question}' ===\")\n",
    "\n",
    "    # Step 1: Decompose the main question into sub-questions\n",
    "    print(\"Step 1: Decomposing the query into sub-questions...\")\n",
    "    iterdrag_demonstrations = create_iterdrag_demonstrations()\n",
    "    formatted_demonstrations = \"\\n\\n\".join(\n",
    "        f\"Example {i+1}:\\n{demo}\"\n",
    "        for i, demo in enumerate(iterdrag_demonstrations)\n",
    "    )\n",
    "    decompose_result = decompose_chain.invoke({\n",
    "        \"input\": main_question,\n",
    "        \"demonstrations\": formatted_demonstrations,\n",
    "    })\n",
    "    decompose_answer = decompose_result\n",
    "\n",
    "    # Extract sub-questions using regex\n",
    "    sub_questions = re.findall(r\"Follow up: (.*?)(?=Follow up:|\\n|$)\", decompose_answer, re.DOTALL)\n",
    "    sub_questions = [sq.strip() for sq in sub_questions if sq.strip()]\n",
    "    if not sub_questions:\n",
    "        print(\"No decomposition needed or found. Using standard DRAG approach.\")\n",
    "        return drag_chain.invoke({\"input\": main_question})\n",
    "    print(f\"Decomposed into {len(sub_questions)} sub-questions\")\n",
    "\n",
    "    # Step 2: Answer each sub-question\n",
    "    intermediate_pairs: list[dict[str, str]] = []\n",
    "    for i, sub_question in enumerate(sub_questions):\n",
    "        print(f\"\\nStep 2.{i+1}: Processing sub-question: '{sub_question}'\")\n",
    "\n",
    "        # Generate answer for this sub-question\n",
    "        intermediate_result = intermediate_chain.invoke({\"input\": sub_question})\n",
    "        intermediate_answer = intermediate_result[\"answer\"]\n",
    "\n",
    "        # Extract intermediate answer using regex\n",
    "        intermediate_answer_match = re.search(r\"Intermediate answer: (.*?)$\", intermediate_answer, re.DOTALL)\n",
    "        if intermediate_answer_match:\n",
    "            intermediate_answer = intermediate_answer_match.group(1).strip()\n",
    "\n",
    "        print(f\"Generated intermediate answer: {intermediate_answer[:100]}...\")\n",
    "\n",
    "        # Store the sub-question and its answer\n",
    "        intermediate_pairs.append({\"input\": sub_question, \"answer\": intermediate_answer})\n",
    "\n",
    "    # Step 3: Generate the final answer based on sub-question answers\n",
    "    print(\"\\nStep 3: Generating final answer based on intermediate answers...\")\n",
    "    final_result = final_chain.invoke({\n",
    "        \"input\": main_question,\n",
    "        \"context\": \"\\n\\n\".join(\n",
    "            f\"Sub-question: {pair['input']}\\nIntermediate answer: {pair['answer']}\"\n",
    "            for pair in intermediate_pairs\n",
    "        ),\n",
    "    })\n",
    "    final_answer = final_result\n",
    "\n",
    "    # Extract final answer\n",
    "    final_answer_match = re.search(r\"So the final answer is: (.*?)$\", final_answer, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        final_answer = final_answer_match.group(1).strip()\n",
    "\n",
    "    return {\"input\": main_question, \"answer\": final_answer, \"intermediate\": intermediate_pairs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1HbAsV52bdS"
   },
   "source": [
    "## Comparing RAG Approaches\n",
    "\n",
    "Now that we have all three RAG approaches set up, let's compare there responses same query, this time much more complex, to see the differences.\n",
    "\n",
    "The comparison will help us understand the benefits of each approach and when each might be most appropriate to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FFg5Kf80FLZ"
   },
   "outputs": [],
   "source": [
    "# Run all approaches on the same complex query\n",
    "comparison_query = \"What was the full impact chain of the National Guard's assistance during the pandemic? Specifically, how did their involvement affect volunteer operations, what specific tasks did they perform, and how did this ultimately translate to community impact in terms of food distribution capabilities and reach?\"\n",
    "\n",
    "print(\"\\n=== Standard RAG ===\")\n",
    "standard_result = rag_chain.invoke({\"input\": comparison_query})\n",
    "print(standard_result[\"answer\"])\n",
    "\n",
    "print(\"\\n=== DRAG ===\")\n",
    "drag_result = drag_chain.invoke({\"input\": comparison_query})\n",
    "print(drag_result[\"answer\"])\n",
    "\n",
    "print(\"\\n=== IterDRAG ===\")\n",
    "iterdrag_result = iterative_drag(comparison_query)\n",
    "print(iterdrag_result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMVUrZifhg1h"
   },
   "source": [
    "## Results Comparison and Analysis\n",
    "\n",
    "Here we summarize the performance differences between the three RAG approaches implemented:\n",
    "\n",
    "| Approach | Strengths | Limitations | Best Use Cases |\n",
    "|----------|-----------|-------------|----------------|\n",
    "| Standard RAG | - Simple implementation<br>- Good for straightforward queries<br>- Lower computational requirements | - Limited context utilization<br>- Performance plateaus with more documents<br>- Poor at complex reasoning | - Simple factual queries<br>- When computation is limited<br>- When context is small |\n",
    "| DRAG | - Better context utilization<br>- Improved performance with more documents<br>- Good for moderately complex queries | - Still limited by one-step generation<br>- Less effective for multi-hop questions | - Moderate complexity queries<br>- When more documents are available<br>- When in-context examples can be provided |\n",
    "| IterDRAG | - Best for complex queries<br>- Explicit reasoning chains<br>- Most effective use of context | - Highest computational requirements<br>- More complex implementation | - Multi-hop questions<br>- Complex analyses requiring composite reasoning<br>- When maximum performance is needed |\n",
    "\n",
    "As we've seen in our implementation, inference scaling techniques like DRAG and IterDRAG can significantly improve RAG performance, especially for complex queries requiring deep analysis of multiple documents.\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "In this tutorial, we've explored how inference scaling can dramatically improve RAG performance. By strategically allocating additional computation at inference time through techniques like DRAG and IterDRAG, we can achieve substantial gains in response quality for complex queries.\n",
    "\n",
    "Key takeaways:\n",
    "- Inference scaling provides almost linear improvements in RAG performance when optimally configured\n",
    "- DRAG enhances standard RAG by using in-context examples to guide information extraction\n",
    "- IterDRAG further improves performance through query decomposition and interleaved retrieval\n",
    "- These approaches are particularly valuable for complex, multi-hop questions\n",
    "\n",
    "Next steps you might consider:\n",
    "- Experiment with different retrieval models and document preprocessing approaches\n",
    "- Try different prompt formulations for image understanding\n",
    "- Explore parameter optimization to find the ideal settings for your specific use case\n",
    "\n",
    "By applying these inference scaling techniques to your RAG applications, you can achieve significantly better performance on knowledge-intensive tasks without changing your underlying models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky. 2024. \"Inference Scaling for Long-Context Retrieval Augmented Generation\". [arXiv:2410.04343](https://arxiv.org/abs/2410.04343).\n",
    "\n",
    "Reasoning in Granite 3.2 using inference scaling. 2025. IBM Research Blog. https://research.ibm.com/blog/inference-scaling-reasoning-ai-model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
