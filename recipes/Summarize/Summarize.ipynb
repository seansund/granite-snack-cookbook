{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization\n",
    "\n",
    "This notebook demonstrates an application of long document summarization techniques to a work of literature using Granite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Granite utils provides some helpful functions for recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    langchain_community \\\n",
    "    transformers \\\n",
    "    langchain_huggingface \\\n",
    "    replicate \\\n",
    "    docling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model\n",
    "\n",
    "Select a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-3.2-8b-instruct\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 2000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.75,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Download a book\n",
    "\n",
    "Here we fetch H.D. Thoreau's \"Walden\" from [Project Gutenberg](https://www.gutenberg.org/) for summarization.\n",
    "\n",
    "We have to chunk the book text so that chunks will fit in the context window size of the AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "### Count the tokens\n",
    "\n",
    "Before sending our book chunks to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the [`granite-3.2-8b-instruct`](https://huggingface.co/ibm-granite/granite-3.2-8b-instruct) model, which has a context window of 128K tokens.\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model.\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Summaries\n",
    "\n",
    "Here we use a hierarchical abstractive summarization technique to adapt to the context length of the model. Our approach uses [Docling](https://docling-project.github.io/docling/) to understand the document's structure, chunk the document into text passages, and group the text passages by chapter which we can then summarize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Iterator, Callable\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "from docling_core.transforms.chunker.base import BaseChunk\n",
    "\n",
    "def chunk_document(source: str, *, dropwhile: Callable[[BaseChunk], bool] = lambda c: False, takewhile: Callable[[BaseChunk], bool] = lambda c: True) -> Iterator[BaseChunk]:\n",
    "    \"\"\"Read the document and perform a hierarchical chunking\"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    chunks = HierarchicalChunker().chunk(converter.convert(source=source).document)\n",
    "    return itertools.takewhile(takewhile, itertools.dropwhile(dropwhile, chunks))\n",
    "\n",
    "def merge_chunks(chunks: Iterator[BaseChunk], *, headings: Callable[[BaseChunk], list[str]] = lambda c: c.meta.headings) -> Iterator[dict[str, str]]:\n",
    "    \"\"\"Merge chunks having the same headings\"\"\"\n",
    "    prior_headings: list[str] | None = None\n",
    "    document: dict[str, str] = {}\n",
    "    for chunk in chunks:\n",
    "        text = chunk.text.replace('\\r\\n', '\\n')\n",
    "        current_headings = headings(chunk)\n",
    "        if prior_headings != current_headings:\n",
    "            if document:\n",
    "                yield document\n",
    "            prior_headings = current_headings\n",
    "            document = {'title': \" - \".join(current_headings), 'text': text}\n",
    "        else:\n",
    "            document['text'] += f\"\\n\\n{text}\"\n",
    "    if document:\n",
    "        yield document\n",
    "\n",
    "def chunk_dropwhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore front matter prior to the book start\"\"\"\n",
    "    return \"WALDEN\" not in chunk.meta.headings\n",
    "\n",
    "def chunk_takewhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore remaining chunks once we see this heading\"\"\"\n",
    "    return \"ON THE DUTY OF CIVIL DISOBEDIENCE\" not in chunk.meta.headings\n",
    "\n",
    "def chunk_headings(chunk: BaseChunk) -> list[str]:\n",
    "    \"\"\"Use the h1 and h2 (chapter) headings\"\"\"\n",
    "    return chunk.meta.headings[:2]\n",
    "\n",
    "documents: list[dict[str, str]] = list(merge_chunks(\n",
    "    chunk_document(\n",
    "        \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\",\n",
    "        dropwhile=chunk_dropwhile,\n",
    "        takewhile=chunk_takewhile,\n",
    "    ),\n",
    "    headings=chunk_headings,\n",
    "))\n",
    "\n",
    "print(f\"{len(documents)} documents created\")\n",
    "print(f\"Max document size: {max(len(tokenizer.tokenize(document['text'])) for document in documents)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the chunks\n",
    "\n",
    "Here we define a method to generate a response using a list of documents and a user prompt about those documents. \n",
    "\n",
    "We create the prompt according to the [Granite Prompting Guide](https://www.ibm.com/granite/docs/models/granite/#chat-template) and provide the documents using the `documents` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(user_prompt: str, documents: list[dict[str, str]]):\n",
    "    \"\"\"Use the chat template to format the prompt\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }],\n",
    "        documents=documents, # This uses the documents support in the Granite chat template\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Input size: {len(tokenizer.tokenize(prompt))} tokens\")\n",
    "    output = model.invoke(prompt)\n",
    "    print(f\"Output size: {len(tokenizer.tokenize(output))} tokens\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chapter, we create a separate summary. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_env_var('GRANITE_TESTING', 'false').lower() == 'true':\n",
    "    documents = documents[:5] # shorten testing work\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Using only the the book chapter document, compose a summary of the book chapter.\n",
    "Your response should only include the summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "summaries: list[dict[str, str]] = []\n",
    "\n",
    "for document in documents:\n",
    "    print(f\"============================= {document['title']} =============================\")\n",
    "    output = generate(user_prompt, [document])\n",
    "    summaries.append({'title': document['title'], 'text': output})\n",
    "\n",
    "print(\"Summary count: \" + str(len(summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Final Summary\n",
    "\n",
    "Now we need to summarize the chapter summaries. We prompt the model to create a unified summary of the chapter summaries we previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\\\n",
    "Using only the book chapter summary documents, compose a single, unified summary of the book.\n",
    "Your response should only include the unified summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "output = generate(user_prompt, summaries)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now summarized a document larger than the AI model's context window length by breaking the document down into smaller pieces to summarize and then summarizing those summaries."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
